{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-03T01:59:52.748897Z",
     "start_time": "2020-11-03T01:59:52.285536Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "import glob "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-03T01:59:53.361895Z",
     "start_time": "2020-11-03T01:59:52.750899Z"
    }
   },
   "outputs": [],
   "source": [
    "path = '/Users/labbot/Documents/metis_bootcamp/project04/data/pitbulls'\n",
    "all_files = glob.glob(path + \"/*.csv\")\n",
    "\n",
    "li = []\n",
    "\n",
    "for filename in all_files:\n",
    "    one_df = pd.read_csv(filename, index_col=None, header=0)\n",
    "    li.append(one_df)\n",
    "\n",
    "df = pd.concat(li, axis=0, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-03T01:59:53.371036Z",
     "start_time": "2020-11-03T01:59:53.364139Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(141515, 20)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-03T01:59:53.723195Z",
     "start_time": "2020-11-03T01:59:53.372924Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "body                      127239\n",
       "score_hidden                   0\n",
       "archived                       0\n",
       "name                           0\n",
       "author                     21046\n",
       "author_flair_text            150\n",
       "downs                          0\n",
       "created_utc               141141\n",
       "subreddit_id                   1\n",
       "link_id                    20697\n",
       "parent_id                  66056\n",
       "score                        286\n",
       "retrieved_on              140825\n",
       "controversiality               2\n",
       "gilded                         3\n",
       "id                        141515\n",
       "subreddit                      1\n",
       "ups                            0\n",
       "distinguished                  1\n",
       "author_flair_css_class         0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-03T01:59:53.779239Z",
     "start_time": "2020-11-03T01:59:53.725114Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "body                           1\n",
       "score_hidden              141515\n",
       "archived                  141515\n",
       "name                      141515\n",
       "author                         0\n",
       "author_flair_text         137806\n",
       "downs                     141515\n",
       "created_utc                    0\n",
       "subreddit_id                   0\n",
       "link_id                        0\n",
       "parent_id                      0\n",
       "score                          0\n",
       "retrieved_on                   0\n",
       "controversiality               0\n",
       "gilded                         0\n",
       "id                             0\n",
       "subreddit                      0\n",
       "ups                       141515\n",
       "distinguished             141376\n",
       "author_flair_css_class    141515\n",
       "dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-05T00:56:21.293797Z",
     "start_time": "2020-11-05T00:56:20.824693Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20697, 5)"
      ]
     },
     "execution_count": 285,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# need to drop the single row where comment body is missing\n",
    "df = df.dropna(subset = ['body'])\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-03T02:00:08.159936Z",
     "start_time": "2020-11-03T02:00:07.998889Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20697, 2)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# document = all comments on a given post\n",
    "df = df.groupby(['link_id'], as_index = False).agg({'body': ' '.join})\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-05T22:34:30.731749Z",
     "start_time": "2020-11-05T22:34:30.133524Z"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# check the length of newly created documents\n",
    "df['doc_length'] = df['body'].str.split().str.len()\n",
    "df['doc_length'][df['doc_length'] < 1500].hist(bins=100);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-03T02:48:37.564893Z",
     "start_time": "2020-11-03T02:48:36.134509Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/labbot/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /Users/labbot/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import string\n",
    "\n",
    "from sklearn.base import TransformerMixin, BaseEstimator\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "import spacy\n",
    "import en_core_web_sm\n",
    "nlp = en_core_web_sm.load()\n",
    "\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Stepwise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-03T02:19:53.835530Z",
     "start_time": "2020-11-03T02:19:53.830399Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# first create a copy of the text column so we can compare to it if needed\n",
    "df['body_orig'] = df['body']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Remove numbers, capital letters, punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-03T02:20:30.761616Z",
     "start_time": "2020-11-03T02:20:29.018901Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>link_id</th>\n",
       "      <th>body</th>\n",
       "      <th>body_orig</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>20692</th>\n",
       "      <td>t3_ei9kfu</td>\n",
       "      <td>i’ll snuggle you  jenny</td>\n",
       "      <td>I’ll snuggle you, Jenny!!!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20693</th>\n",
       "      <td>t3_ei9omv</td>\n",
       "      <td>that lip     omg so cute    eeeeeee  so handso...</td>\n",
       "      <td>That lip!!!! Omg so cute!!! Eeeeeee. So handso...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20694</th>\n",
       "      <td>t3_ei9xrt</td>\n",
       "      <td>i love the little spots</td>\n",
       "      <td>I love the little spots :)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20695</th>\n",
       "      <td>t3_eia3h0</td>\n",
       "      <td>���� that’s going to be me too  admittedly  it...</td>\n",
       "      <td>���� that’s going to be me too! Admittedly, it...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20696</th>\n",
       "      <td>t3_eiaf39</td>\n",
       "      <td>nuggets mine was called  puppy  for the first ...</td>\n",
       "      <td>Nuggets Mine was called \"puppy\" for the first ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         link_id                                               body  \\\n",
       "20692  t3_ei9kfu                         i’ll snuggle you  jenny      \n",
       "20693  t3_ei9omv  that lip     omg so cute    eeeeeee  so handso...   \n",
       "20694  t3_ei9xrt                         i love the little spots      \n",
       "20695  t3_eia3h0  ���� that’s going to be me too  admittedly  it...   \n",
       "20696  t3_eiaf39  nuggets mine was called  puppy  for the first ...   \n",
       "\n",
       "                                               body_orig  \n",
       "20692                         I’ll snuggle you, Jenny!!!  \n",
       "20693  That lip!!!! Omg so cute!!! Eeeeeee. So handso...  \n",
       "20694                         I love the little spots :)  \n",
       "20695  ���� that’s going to be me too! Admittedly, it...  \n",
       "20696  Nuggets Mine was called \"puppy\" for the first ...  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alphanumeric = lambda x: re.sub('\\w*\\d\\w*', ' ', x)\n",
    "punc_lower = lambda x: re.sub('[%s]' % re.escape(string.punctuation), ' ', x.lower())\n",
    "breaks = lambda x: re.sub(\"\\n\", \" \", x)\n",
    "\n",
    "df['body'] = df['body'].map(alphanumeric).map(punc_lower).map(breaks)\n",
    "df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-05T22:38:09.560018Z",
     "start_time": "2020-11-05T22:38:09.508274Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# for body in df[\"body\"][0:20]:\n",
    "#     print(body)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Remove Stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-03T02:24:18.719804Z",
     "start_time": "2020-11-03T02:24:18.716849Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Create list of stopwords\n",
    "stop_words = spacy.lang.en.stop_words.STOP_WORDS\n",
    "# figure out best way to add stopwords like: �"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-03T02:35:00.258829Z",
     "start_time": "2020-11-03T02:34:58.449758Z"
    },
    "hidden": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/labbot/opt/anaconda3/lib/python3.8/site-packages/sklearn/feature_extraction/text.py:383: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ll', 've'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>aa</th>\n",
       "      <th>aaaaand</th>\n",
       "      <th>aaaah</th>\n",
       "      <th>aaaw</th>\n",
       "      <th>aaawww</th>\n",
       "      <th>aaww</th>\n",
       "      <th>ab</th>\n",
       "      <th>abandon</th>\n",
       "      <th>abandoned</th>\n",
       "      <th>abandoning</th>\n",
       "      <th>...</th>\n",
       "      <th>zombie</th>\n",
       "      <th>zone</th>\n",
       "      <th>zoo</th>\n",
       "      <th>zoom</th>\n",
       "      <th>zoomed</th>\n",
       "      <th>zoomie</th>\n",
       "      <th>zoomies</th>\n",
       "      <th>zooming</th>\n",
       "      <th>zooms</th>\n",
       "      <th>zyrtec</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 11725 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   aa  aaaaand  aaaah  aaaw  aaawww  aaww  ab  abandon  abandoned  abandoning  \\\n",
       "0   0        0      0     0       0     0   0        0          0           0   \n",
       "1   0        0      0     0       0     0   0        0          0           0   \n",
       "2   0        0      0     0       0     0   0        0          0           0   \n",
       "3   0        0      0     0       0     0   0        0          0           0   \n",
       "4   0        0      0     0       0     0   0        0          0           0   \n",
       "\n",
       "   ...  zombie  zone  zoo  zoom  zoomed  zoomie  zoomies  zooming  zooms  \\\n",
       "0  ...       0     0    0     0       0       0        0        0      0   \n",
       "1  ...       0     0    0     0       0       0        0        0      0   \n",
       "2  ...       0     0    0     0       0       0        0        0      0   \n",
       "3  ...       0     0    0     0       0       0        0        0      0   \n",
       "4  ...       0     0    0     0       0       0        0        0      0   \n",
       "\n",
       "   zyrtec  \n",
       "0       0  \n",
       "1       0  \n",
       "2       0  \n",
       "3       0  \n",
       "4       0  \n",
       "\n",
       "[5 rows x 11725 columns]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check what feature space looks like at this point\n",
    "cv = CountVectorizer(stop_words=stop_words, min_df=5)\n",
    "doc_word = cv.fit_transform(df[\"body\"])\n",
    "cv_dv = pd.DataFrame(doc_word.toarray(),columns=cv.get_feature_names())\n",
    "cv_dv.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-03T02:45:24.059062Z",
     "start_time": "2020-11-03T02:45:24.056163Z"
    },
    "hidden": true
   },
   "source": [
    "### Lemmatize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-03T02:47:29.529138Z",
     "start_time": "2020-11-03T02:47:29.524735Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "word_tokenizer = lambda x: word_tokenize(x)\n",
    "lemmatize = lambda x: \" \".join([lemmatizer.lemmatize(w) for w in x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-03T02:49:00.631028Z",
     "start_time": "2020-11-03T02:48:43.209488Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>link_id</th>\n",
       "      <th>body</th>\n",
       "      <th>body_orig</th>\n",
       "      <th>Lemmatized</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>t3_8zrnvs</td>\n",
       "      <td>dude  she s beautiful</td>\n",
       "      <td>Dude, she's beautiful!</td>\n",
       "      <td>dude she s beautiful</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>t3_925zut</td>\n",
       "      <td>lol  angry     fair enough  perish sorry thoug...</td>\n",
       "      <td>Lol, angry? ;) Fair enough. Perish Sorry thoug...</td>\n",
       "      <td>lol angry fair enough perish sorry thought you...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>t3_929abm</td>\n",
       "      <td>not interested in your content not interested ...</td>\n",
       "      <td>Not interested in your content not interested ...</td>\n",
       "      <td>not interested in your content not interested ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>t3_9cmyda</td>\n",
       "      <td>thanks for your information in  baby allergies...</td>\n",
       "      <td>thanks for your information in [baby allergies...</td>\n",
       "      <td>thanks for your information in baby allergy an...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>t3_9dv6mo</td>\n",
       "      <td>looks like all the fat in his body went to his...</td>\n",
       "      <td>Looks like all the fat in his body went to his...</td>\n",
       "      <td>look like all the fat in his body went to his ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20692</th>\n",
       "      <td>t3_ei9kfu</td>\n",
       "      <td>i’ll snuggle you  jenny</td>\n",
       "      <td>I’ll snuggle you, Jenny!!!</td>\n",
       "      <td>i ’ ll snuggle you jenny</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20693</th>\n",
       "      <td>t3_ei9omv</td>\n",
       "      <td>that lip     omg so cute    eeeeeee  so handso...</td>\n",
       "      <td>That lip!!!! Omg so cute!!! Eeeeeee. So handso...</td>\n",
       "      <td>that lip omg so cute eeeeeee so handsome also ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20694</th>\n",
       "      <td>t3_ei9xrt</td>\n",
       "      <td>i love the little spots</td>\n",
       "      <td>I love the little spots :)</td>\n",
       "      <td>i love the little spot</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20695</th>\n",
       "      <td>t3_eia3h0</td>\n",
       "      <td>���� that’s going to be me too  admittedly  it...</td>\n",
       "      <td>���� that’s going to be me too! Admittedly, it...</td>\n",
       "      <td>���� that ’ s going to be me too admittedly it...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20696</th>\n",
       "      <td>t3_eiaf39</td>\n",
       "      <td>nuggets mine was called  puppy  for the first ...</td>\n",
       "      <td>Nuggets Mine was called \"puppy\" for the first ...</td>\n",
       "      <td>nugget mine wa called puppy for the first two ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>20697 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         link_id                                               body  \\\n",
       "0      t3_8zrnvs                             dude  she s beautiful    \n",
       "1      t3_925zut  lol  angry     fair enough  perish sorry thoug...   \n",
       "2      t3_929abm  not interested in your content not interested ...   \n",
       "3      t3_9cmyda  thanks for your information in  baby allergies...   \n",
       "4      t3_9dv6mo  looks like all the fat in his body went to his...   \n",
       "...          ...                                                ...   \n",
       "20692  t3_ei9kfu                         i’ll snuggle you  jenny      \n",
       "20693  t3_ei9omv  that lip     omg so cute    eeeeeee  so handso...   \n",
       "20694  t3_ei9xrt                         i love the little spots      \n",
       "20695  t3_eia3h0  ���� that’s going to be me too  admittedly  it...   \n",
       "20696  t3_eiaf39  nuggets mine was called  puppy  for the first ...   \n",
       "\n",
       "                                               body_orig  \\\n",
       "0                                 Dude, she's beautiful!   \n",
       "1      Lol, angry? ;) Fair enough. Perish Sorry thoug...   \n",
       "2      Not interested in your content not interested ...   \n",
       "3      thanks for your information in [baby allergies...   \n",
       "4      Looks like all the fat in his body went to his...   \n",
       "...                                                  ...   \n",
       "20692                         I’ll snuggle you, Jenny!!!   \n",
       "20693  That lip!!!! Omg so cute!!! Eeeeeee. So handso...   \n",
       "20694                         I love the little spots :)   \n",
       "20695  ���� that’s going to be me too! Admittedly, it...   \n",
       "20696  Nuggets Mine was called \"puppy\" for the first ...   \n",
       "\n",
       "                                              Lemmatized  \n",
       "0                                   dude she s beautiful  \n",
       "1      lol angry fair enough perish sorry thought you...  \n",
       "2      not interested in your content not interested ...  \n",
       "3      thanks for your information in baby allergy an...  \n",
       "4      look like all the fat in his body went to his ...  \n",
       "...                                                  ...  \n",
       "20692                           i ’ ll snuggle you jenny  \n",
       "20693  that lip omg so cute eeeeeee so handsome also ...  \n",
       "20694                             i love the little spot  \n",
       "20695  ���� that ’ s going to be me too admittedly it...  \n",
       "20696  nugget mine wa called puppy for the first two ...  \n",
       "\n",
       "[20697 rows x 4 columns]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['Lemmatized'] = df['body'].apply(word_tokenizer).apply(lemmatize)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-04T00:20:25.922170Z",
     "start_time": "2020-11-04T00:20:23.844346Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/labbot/opt/anaconda3/lib/python3.8/site-packages/sklearn/feature_extraction/text.py:383: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ll', 've'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>aa</th>\n",
       "      <th>aaaaand</th>\n",
       "      <th>aaaah</th>\n",
       "      <th>aaaw</th>\n",
       "      <th>aaawww</th>\n",
       "      <th>aaww</th>\n",
       "      <th>ab</th>\n",
       "      <th>abandon</th>\n",
       "      <th>abandoned</th>\n",
       "      <th>abandoning</th>\n",
       "      <th>...</th>\n",
       "      <th>zola</th>\n",
       "      <th>zombie</th>\n",
       "      <th>zone</th>\n",
       "      <th>zoo</th>\n",
       "      <th>zoom</th>\n",
       "      <th>zoomed</th>\n",
       "      <th>zoomie</th>\n",
       "      <th>zoomies</th>\n",
       "      <th>zooming</th>\n",
       "      <th>zyrtec</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 10378 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   aa  aaaaand  aaaah  aaaw  aaawww  aaww  ab  abandon  abandoned  abandoning  \\\n",
       "0   0        0      0     0       0     0   0        0          0           0   \n",
       "1   0        0      0     0       0     0   0        0          0           0   \n",
       "2   0        0      0     0       0     0   0        0          0           0   \n",
       "3   0        0      0     0       0     0   0        0          0           0   \n",
       "4   0        0      0     0       0     0   0        0          0           0   \n",
       "\n",
       "   ...  zola  zombie  zone  zoo  zoom  zoomed  zoomie  zoomies  zooming  \\\n",
       "0  ...     0       0     0    0     0       0       0        0        0   \n",
       "1  ...     0       0     0    0     0       0       0        0        0   \n",
       "2  ...     0       0     0    0     0       0       0        0        0   \n",
       "3  ...     0       0     0    0     0       0       0        0        0   \n",
       "4  ...     0       0     0    0     0       0       0        0        0   \n",
       "\n",
       "   zyrtec  \n",
       "0       0  \n",
       "1       0  \n",
       "2       0  \n",
       "3       0  \n",
       "4       0  \n",
       "\n",
       "[5 rows x 10378 columns]"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check what feature space looks like at this point\n",
    "cv = CountVectorizer(stop_words=stop_words, min_df=5)\n",
    "doc_word = cv.fit_transform(df[\"Lemmatized\"])\n",
    "cv_dv = pd.DataFrame(doc_word.toarray(),columns=cv.get_feature_names())\n",
    "cv_dv.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-03T02:55:37.929820Z",
     "start_time": "2020-11-03T02:55:37.926373Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "punctuations = string.punctuation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-03T02:55:41.178772Z",
     "start_time": "2020-11-03T02:55:41.173811Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "punctuations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OOP Take 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-03T22:15:53.597855Z",
     "start_time": "2020-11-03T22:15:53.550581Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-04T00:51:21.067306Z",
     "start_time": "2020-11-04T00:51:20.821297Z"
    }
   },
   "outputs": [],
   "source": [
    "from SpacyPreprocessor import SpacyPreprocessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-05T01:14:06.935072Z",
     "start_time": "2020-11-05T01:14:06.880953Z"
    }
   },
   "outputs": [],
   "source": [
    "# Create list of stopwords\n",
    "stop_words = spacy_model.Defaults.stop_words\n",
    "\n",
    "# list of words that are germane to subject that I'll treat as stop words\n",
    "custom_stopwords = ['pit','bull','pitbull','dog','breed','puppy','good','love','thank',\n",
    "                   'boy','girl','people','owner','like','look','know','luck','sorry','loss','lose','look',\n",
    "                   'lol','haha','pron','thing', 'com','https','think','remove','want','birthday','time','year',\n",
    "                    'day','oh','sure', 'post','comment','delete','sub','subreddit', 'hi', 'utm', 'reddit', 'www', \n",
    "                    'reddit', 'www', 'share', 'bot', 'amp', 'utm', 'faq', 'amp', 'imgur', 'omg', 'ah', 'hello', \n",
    "                    'try', 'right', 'find','need','come','way','use','lot','imgur gallery','imgur','gallery','gif',\n",
    "                   'app','yes','let','minute','month', 'week','year','new','wow']\n",
    "\n",
    "\n",
    "# add custom stopwords to stopwords list\n",
    "for s in custom_stopwords:\n",
    "    stop_words.add(s)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-05T00:32:01.973360Z",
     "start_time": "2020-11-05T00:30:26.301282Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "20697it [01:35, 217.71it/s]\n"
     ]
    }
   ],
   "source": [
    "spacy_model = SpacyPreprocessor.load_model()\n",
    "preprocessor = SpacyPreprocessor(spacy_model=spacy_model, lemmatize=True, remove_numbers=True, \n",
    "                                 remove_stopwords=False, remove_special=True, \n",
    "                                 pos_to_remove=['PROPN','ADP','SYM','NUM','AUX'])\n",
    "df['spacy_pipe'] = preprocessor.preprocess_text_list(list(df['body_orig']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topic Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-05T01:54:09.130557Z",
     "start_time": "2020-11-05T01:54:09.081154Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import NMF, TruncatedSVD, LatentDirichletAllocation\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "\n",
    "\n",
    "#from gensim import matutils\n",
    "\n",
    "from TopicModeling import topic_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-05T00:32:11.091341Z",
     "start_time": "2020-11-05T00:32:08.117649Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/labbot/opt/anaconda3/lib/python3.8/site-packages/sklearn/feature_extraction/text.py:383: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ll', 've'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Topic  0\n",
      "work, little, baby, old, home, happy, sweet, tell, life, bad, pup, big, beautiful, great, feel\n",
      "\n",
      "Topic  1\n",
      "toy, destroy, chew, ball, black, buy, bone, long, month, antler, tooth, chewer, rope, tough, minute\n",
      "\n",
      "Topic  2\n",
      "collar, training, work, train, harness, walk, attack, pull, animal, aggressive, prong, leash, bite, prong collar, bad\n",
      "\n",
      "Topic  3\n",
      "collar, harness, prong, pull, walk, prong collar, food, training, vet, pain, reinforcement, work, happy, hope, positive\n",
      "\n",
      "Topic  4\n",
      "food, vet, ear, allergy, grain, skin, help, diet, free, grain free, chicken, raw, feed, eat, issue\n",
      "\n",
      "Topic  5\n",
      "ear, collar, cute, crop, tail, adorable, harness, face, beautiful, prong, cut, pull, reason, prong collar, yes\n",
      "\n",
      "Topic  6\n",
      "cute, walk, face, pibble, little, adorable, hike, bed, food, pittie, sit, big, cat, blanket, mix\n",
      "\n",
      "Topic  7\n",
      "home, shelter, ear, rescue, cat, adopt, happy, new, crate, training, foster, month, breeder, bed, tail\n",
      "\n",
      "Topic  8\n",
      "ear, walk, leash, baby, feel, hope, run, send, let, play, poor, tail, park, soon, hike\n",
      "\n",
      "Topic  9\n",
      "happy, park, walk, leash, happy happy, attack, aggressive, treat, food, hike, ear, socialize, train, bite, smile\n",
      "\n",
      "Topic  10\n",
      "walk, hike, shelter, mix, hiking, mile, vet, live, rescue, leash, nail, long, breeder, trail, boot\n",
      "\n",
      "Topic  11\n",
      "cat, beautiful, friend, hike, walk, life, ear, old, heart, food, park, hiking, play, great, mile\n",
      "\n",
      "Topic  12\n",
      "baby, cat, sweet, leash, shelter, poor, hope, soon, play, aggressive, adopt, attack, recovery, poor baby, little\n",
      "\n",
      "Topic  13\n",
      "park, beautiful, advice, socialize, pup, help, training, old, socialization, opinion, treat, train, rescue, family, eye\n",
      "\n",
      "Topic  14\n",
      "nail, trim, cat, treat, nail trim, beautiful, animal, harness, life, hold, trim nail, clip, little, hate, pup\n"
     ]
    }
   ],
   "source": [
    "pitbull_topics = topic_model(data=df['spacy_pipe'],\n",
    "                             vectorizer=CountVectorizer(stop_words=stop_words,min_df=10,ngram_range=(1, 2)),\n",
    "                            model='lsa',num_topics=15)\n",
    "pitbull_topics_cv_lsa = pitbull_topics.vectorize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-05T00:32:44.779482Z",
     "start_time": "2020-11-05T00:32:41.302112Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Topic  0\n",
      "old, work, little, pibble, home, pup, great, life, walk, tell, let, rescue, big, vet, help\n",
      "\n",
      "Topic  1\n",
      "cute, super cute, super, little, cute cute, cute little, cute pup, pup, ear cute, cute baby, cute pupper, adorable cute, pupper, face cute, pibble\n",
      "\n",
      "Topic  2\n",
      "beautiful, beautiful pup, beautiful beautiful, pup, beautiful baby, picture, beautiful smile, beautiful picture, beautiful eye, beautiful face, coat, color, happy beautiful, wow, beautiful coat\n",
      "\n",
      "Topic  3\n",
      "happy, happy happy, cake, happy pup, happy sweet, beautiful happy, happy beautiful, life, happy face, pup, happy cake, happy handsome, handsome happy, happy baby, baby happy\n",
      "\n",
      "Topic  4\n",
      "handsome, boi, handsome handsome, handsome boi, happy handsome, great, handsome pup, fella, handsome fella, pup, man, handsome happy, looking, guy, handsome guy\n",
      "\n",
      "Topic  5\n",
      "ear, crop, ear cute, floppy, floppy ear, ear ear, crop ear, bat, cute ear, big, mix, ear beautiful, cut, ear crop, bat ear\n",
      "\n",
      "Topic  6\n",
      "adorable, adorable cute, absolutely adorable, little, pibble, pup, absolutely, adorable face, adorable little, cute adorable, cut, face adorable, goodness, adorable pup, kiss\n",
      "\n",
      "Topic  7\n",
      "face, sweet face, kiss, precious, happy face, little, beautiful face, face cute, cute face, face face, pibble, adorable face, little face, seal, cut\n",
      "\n",
      "Topic  8\n",
      "cutie, pie, cutie pie, little, amazing, cutie cute, wow, beauty, snoot, nice, boop, precious, little cutie, wow cutie, ear cutie\n",
      "\n",
      "Topic  9\n",
      "sweet, sweet face, sweet baby, pup, heart, sweet pup, sweet sweet, life, sweet little, hug, pupper, beautiful sweet, angel, hope, happy sweet\n",
      "\n",
      "Topic  10\n",
      "gorgeous, absolutely gorgeous, absolutely, pup, beauty, gorgeous pupper, pupper, great, gorgeous pup, coat, nice, gorgeous beautiful, picture, wow, gorgeous baby\n",
      "\n",
      "Topic  11\n",
      "smile, beautiful smile, great, big, picture, smile smile, great smile, big smile, pittie, pittie smile, pibble, smile happy, happy smile, pibble smile, pic\n",
      "\n",
      "Topic  12\n",
      "eye, beautiful eye, blue, blue eye, color, wow, eye eye, nose, white, little, sweet eye, eye beautiful, eye cute, precious, gorgeous eye\n",
      "\n",
      "Topic  13\n",
      "pretty, pupper, pretty pupper, pretty baby, pretty pretty, pretty beautiful, brindle, mix, color, pretty pittie, pink, pittie, pretty pup, picture, sweetheart\n",
      "\n",
      "Topic  14\n",
      "baby, big, sweet baby, poor, beautiful baby, precious, poor baby, big baby, little, cute baby, pretty baby, old, little baby, kiss, baby baby\n"
     ]
    }
   ],
   "source": [
    "pitbull_topics = topic_model(data=df['spacy_pipe'],\n",
    "                             vectorizer=TfidfVectorizer(stop_words=stop_words,min_df=10,ngram_range=(1, 2)),\n",
    "                            model='nmf',num_topics=15)\n",
    "pitbull_topics_cv_nmf = pitbull_topics.vectorize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-05T00:33:05.790985Z",
     "start_time": "2020-11-05T00:33:02.854662Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/labbot/opt/anaconda3/lib/python3.8/site-packages/sklearn/feature_extraction/text.py:383: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ll', 've'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Topic  0\n",
      "cute, beautiful, happy, face, sweet, baby, adorable, little, pup, ear, handsome, pibble, big, old, eye\n",
      "\n",
      "Topic  1\n",
      "cute, super cute, cute cute, cute little, ear cute, cute pup, super, cute baby, adorable cute, cute pupper, face cute, cute ear, cute face, cute adorable, soooo cute\n",
      "\n",
      "Topic  2\n",
      "beautiful, cute, beautiful pup, beautiful beautiful, gorgeous, beautiful baby, beautiful smile, beautiful eye, beautiful picture, beautiful face, smile, eye, happy beautiful, beautiful pupper, beautiful coat\n",
      "\n",
      "Topic  3\n",
      "handsome, happy, smile, happy happy, face, gorgeous, sweet, boi, happy handsome, cutie, handsome happy, cake, handsome handsome, adorable, happy face\n",
      "\n",
      "Topic  4\n",
      "happy, happy happy, smile, cute, sweet, life, cake, home, happy sweet, beautiful happy, happy beautiful, happy pup, hope, happy face, happy cake\n",
      "\n",
      "Topic  5\n",
      "ear, adorable, happy, smile, face, cutie, sweet, happy happy, crop, ear cute, precious, floppy, floppy ear, ear ear, sweet face\n",
      "\n",
      "Topic  6\n",
      "adorable, face, handsome, sweet, baby, cutie, adorable cute, sweet face, absolutely adorable, eye, smile, adorable face, little, absolutely, kiss\n",
      "\n",
      "Topic  7\n",
      "face, sweet, sweet face, eye, baby, cutie, gorgeous, sweet baby, precious, smile, kiss, beautiful face, snoot, face cute, face sweet\n",
      "\n",
      "Topic  8\n",
      "cutie, smile, eye, gorgeous, pretty, pie, cutie pie, little, nice, wow, beauty, big, precious, pic, amazing\n",
      "\n",
      "Topic  9\n",
      "sweet, baby, sweet baby, pup, poor, heart, hope, poor baby, life, beautiful baby, sweet pup, old, hug, sweet face, pretty\n",
      "\n",
      "Topic  10\n",
      "smile, gorgeous, pretty, eye, picture, big, great, beautiful smile, pupper, absolutely gorgeous, pittie, smile smile, baby, absolutely, nice\n",
      "\n",
      "Topic  11\n",
      "smile, sweet, beautiful smile, great, picture, big, smile smile, great smile, big smile, handsome, pittie, beautiful, pittie smile, pibble, life\n",
      "\n",
      "Topic  12\n",
      "eye, pretty, beautiful eye, smile, blue, pup, precious, blue eye, bed, seal, happy, eye eye, pretty eye, color, treat\n",
      "\n",
      "Topic  13\n",
      "pretty, baby, face, big, pretty baby, pupper, beautiful baby, pretty pupper, big baby, poor, sleep, nice, pretty pretty, poor baby, blanket\n",
      "\n",
      "Topic  14\n",
      "baby, eye, big, bed, precious, poor, yes, beautiful baby, big baby, pibble, little, poor baby, face, sweet baby, blanket\n"
     ]
    }
   ],
   "source": [
    "pitbull_topics = topic_model(data=df['spacy_pipe'],\n",
    "                             vectorizer=TfidfVectorizer(stop_words=stop_words,min_df=10,ngram_range=(1, 2)),\n",
    "                            model='lsa',num_topics=15)\n",
    "pitbull_topics_tfidf_lsa = pitbull_topics.vectorize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-05T00:33:21.442380Z",
     "start_time": "2020-11-05T00:33:17.990083Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/labbot/opt/anaconda3/lib/python3.8/site-packages/sklearn/feature_extraction/text.py:383: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ll', 've'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Topic  0\n",
      "old, work, little, pibble, home, pup, great, life, walk, tell, let, rescue, big, vet, help\n",
      "\n",
      "Topic  1\n",
      "cute, super cute, super, little, cute cute, cute little, cute pup, pup, ear cute, cute baby, cute pupper, adorable cute, pupper, face cute, pibble\n",
      "\n",
      "Topic  2\n",
      "beautiful, beautiful pup, beautiful beautiful, pup, beautiful baby, picture, beautiful smile, beautiful picture, beautiful eye, beautiful face, coat, color, happy beautiful, wow, beautiful coat\n",
      "\n",
      "Topic  3\n",
      "happy, happy happy, cake, happy pup, happy sweet, beautiful happy, happy beautiful, life, happy face, pup, happy cake, happy handsome, handsome happy, happy baby, baby happy\n",
      "\n",
      "Topic  4\n",
      "handsome, boi, handsome handsome, handsome boi, happy handsome, great, handsome pup, fella, handsome fella, pup, man, handsome happy, looking, guy, handsome guy\n",
      "\n",
      "Topic  5\n",
      "ear, crop, ear cute, floppy, floppy ear, ear ear, crop ear, bat, cute ear, big, mix, ear beautiful, cut, ear crop, bat ear\n",
      "\n",
      "Topic  6\n",
      "adorable, adorable cute, absolutely adorable, little, pibble, pup, absolutely, adorable face, adorable little, cute adorable, cut, face adorable, goodness, adorable pup, kiss\n",
      "\n",
      "Topic  7\n",
      "face, sweet face, kiss, precious, happy face, little, beautiful face, face cute, cute face, face face, pibble, adorable face, little face, seal, cut\n",
      "\n",
      "Topic  8\n",
      "cutie, pie, cutie pie, little, amazing, cutie cute, wow, beauty, snoot, nice, boop, precious, little cutie, wow cutie, ear cutie\n",
      "\n",
      "Topic  9\n",
      "sweet, sweet face, sweet baby, pup, heart, sweet pup, sweet sweet, life, sweet little, hug, pupper, beautiful sweet, angel, hope, happy sweet\n",
      "\n",
      "Topic  10\n",
      "gorgeous, absolutely gorgeous, absolutely, pup, beauty, gorgeous pupper, pupper, great, gorgeous pup, coat, nice, gorgeous beautiful, picture, wow, gorgeous baby\n",
      "\n",
      "Topic  11\n",
      "smile, beautiful smile, great, big, picture, smile smile, great smile, big smile, pittie, pittie smile, pibble, smile happy, happy smile, pibble smile, pic\n",
      "\n",
      "Topic  12\n",
      "eye, beautiful eye, blue, blue eye, color, wow, eye eye, nose, white, little, sweet eye, eye beautiful, eye cute, precious, gorgeous eye\n",
      "\n",
      "Topic  13\n",
      "pretty, pupper, pretty pupper, pretty baby, pretty pretty, pretty beautiful, brindle, mix, color, pretty pittie, pink, pittie, pretty pup, picture, sweetheart\n",
      "\n",
      "Topic  14\n",
      "baby, big, sweet baby, poor, beautiful baby, precious, poor baby, big baby, little, cute baby, pretty baby, old, little baby, kiss, baby baby\n"
     ]
    }
   ],
   "source": [
    "pitbull_topics = topic_model(data=df['spacy_pipe'],\n",
    "                             vectorizer=TfidfVectorizer(stop_words=stop_words,min_df=10,ngram_range=(1, 2)),\n",
    "                            model='nmf',num_topics=15)\n",
    "pitbull_topics_tfidf_nmf = pitbull_topics.vectorize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-05T00:36:41.244783Z",
     "start_time": "2020-11-05T00:35:22.065480Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/labbot/opt/anaconda3/lib/python3.8/site-packages/sklearn/feature_extraction/text.py:383: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ll', 've'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Topic  0\n",
      "toy, ball, chew, destroy, play, bone, buy, stuff, long, minute, rope, tooth, big, throw, treat\n",
      "\n",
      "Topic  1\n",
      "aggressive, animal, train, park, leash, attack, bite, walk, work, happen, play, let, live, bad, training\n",
      "\n",
      "Topic  2\n",
      "food, vet, collar, help, allergy, work, skin, issue, treat, bad, feed, chicken, free, start, diet\n",
      "\n",
      "Topic  3\n",
      "ear, mix, cute, big, adorable, little, baby, cutie, lab, head, pittie, crop, old, pup, tail\n",
      "\n",
      "Topic  4\n",
      "eat, wear, car, harness, leg, surgery, little, walk, work, run, fit, yeah, pull, butter, long\n",
      "\n",
      "Topic  5\n",
      "sit, treat, pet, yes, pibble, work, little, lap, head, boop, attention, new, let, snoot, door\n",
      "\n",
      "Topic  6\n",
      "beautiful, happy, gorgeous, pretty, sweet, baby, pup, eye, smile, handsome, old, face, picture, pupper, absolutely\n",
      "\n",
      "Topic  7\n",
      "water, nail, walk, bath, hate, snow, rain, run, foot, paw, outside, trim, grass, hot, long\n",
      "\n",
      "Topic  8\n",
      "old, walk, month, home, play, work, training, start, crate, house, leave, new, week, great, long\n",
      "\n",
      "Topic  9\n",
      "face, cute, sweet, pibble, little, eye, belly, seal, adorable, baby, white, rub, land, sweet face, super\n",
      "\n",
      "Topic  10\n",
      "life, hope, baby, sweet, heart, feel, send, happy, hug, poor, beautiful, glad, friend, wish, pup\n",
      "\n",
      "Topic  11\n",
      "bed, sleep, pibble, couch, blanket, lay, pillow, little, cute, night, cuddle, wake, pittie, sit, fart\n",
      "\n",
      "Topic  12\n",
      "home, shelter, cat, rescue, adopt, foster, work, animal, care, live, pup, breeder, hope, let, house\n",
      "\n",
      "Topic  13\n",
      "bad, mean, animal, ban, tell, hate, attack, human, child, point, kill, fact, fuck, actually, dangerous\n",
      "\n",
      "Topic  14\n",
      "handsome, cute, smile, adorable, picture, great, pup, face, pic, eye, beautiful, little, big, photo, nose\n"
     ]
    }
   ],
   "source": [
    "pitbull_topics = topic_model(data=df['spacy_pipe'],\n",
    "                             vectorizer=CountVectorizer(stop_words=stop_words,min_df=10,ngram_range=(1, 2)),\n",
    "                            model='lda',num_topics=15)\n",
    "pitbull_topics_cv_lda = pitbull_topics.get_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-05T00:34:16.452625Z",
     "start_time": "2020-11-05T00:33:39.200520Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/labbot/opt/anaconda3/lib/python3.8/site-packages/sklearn/feature_extraction/text.py:383: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ll', 've'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Topic  0\n",
      "beautiful, pupper, seal, baby, great, picture, lovely, gorgeous, land, land seal, great picture, beautiful beautiful, face, sweet, beautiful baby\n",
      "\n",
      "Topic  1\n",
      "adorable, sweet, cute, handsome, cut, heart, hey, cool, smile, cutie, absolutely, gorgeous, fella, ham, eye\n",
      "\n",
      "Topic  2\n",
      "pretty, sploot, beauty, cuddle, beautiful, thanks, eye, space, pibble, sweater, got, shot, personal space, cute, nap\n",
      "\n",
      "Topic  3\n",
      "work, little, home, pibble, old, walk, rescue, pup, toy, baby, big, play, let, bed, great\n",
      "\n",
      "Topic  4\n",
      "cutie, sleep, big, cute, cake, baby, sweetie, awww, shit, lip, cover, sweet, happen, happy cake, nickname\n",
      "\n",
      "Topic  5\n",
      "pic, ball, water, monster, true, couch, friend, job, sad, sleepy, cute, classic, habitat, great pic, soooo\n",
      "\n",
      "Topic  6\n",
      "cute, face, precious, snoot, boi, boop, sweet, tongue, sweet face, twin, ear, little, gosh, adorable, handsome\n",
      "\n",
      "Topic  7\n",
      "eye, looking, silly, guy, grow, looking pup, old, red, melt, pitty, bandana, french, young, vote, park\n",
      "\n",
      "Topic  8\n",
      "gorgeous, title, man, pretty, buddy, rule, cow, cute, collar, totally, gif, picture, pitbulls, app, mix\n",
      "\n",
      "Topic  9\n",
      "ear, perfect, crop, exactly, floppy, ha, pillow, beautiful, amazing, cute, doggo, eye, floppy ear, unit, god\n",
      "\n",
      "Topic  10\n",
      "happy, face, cute, smile, beautiful, nail, adorable, sweet, super, pup, old, handsome, ear, cutie, little\n",
      "\n",
      "Topic  11\n",
      "food, allergy, skin, vet, butter, peanut, snuggle, sit, eat, peanut butter, chicken, treat, grain, diet, oil\n",
      "\n",
      "Topic  12\n",
      "belly, little, nose, pink, white, rub, goodness, spot, belly rub, black, blue, eye, cute, big, pibble\n",
      "\n",
      "Topic  13\n",
      "handsome, hope, life, poor, hug, send, wish, baby, surgery, sweet, happy, recovery, glad, deserve, beautiful\n",
      "\n",
      "Topic  14\n",
      "smile, mix, pup, beautiful, yes, antibsl, awesome, beautiful pup, lab, nice, tell, blep, dna, cute, handsome\n"
     ]
    }
   ],
   "source": [
    "pitbull_topics = topic_model(data=df['spacy_pipe'],\n",
    "                             vectorizer=TfidfVectorizer(stop_words=stop_words,min_df=10,ngram_range=(1, 2)),\n",
    "                            model='lda',num_topics=15)\n",
    "pitbull_topics_tfidf_lda = pitbull_topics.get_topics()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-05T01:21:48.433589Z",
     "start_time": "2020-11-05T01:21:48.317140Z"
    }
   },
   "source": [
    "### More Aggressive Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-05T01:21:48.270280Z",
     "start_time": "2020-11-05T01:20:21.774193Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "20697it [01:26, 239.43it/s]\n"
     ]
    }
   ],
   "source": [
    "# try with verbs and adjectives removed\n",
    "tighter_preprocessor = SpacyPreprocessor(spacy_model=spacy_model, lemmatize=True, remove_numbers=True, \n",
    "                                 remove_stopwords=False, remove_special=True, \n",
    "                                 pos_to_remove=['PROPN','ADP','SYM','NUM','AUX','VERB','ADJ'])\n",
    "\n",
    "tighter_data = tighter_preprocessor.preprocess_text_list(list(df['body_orig']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-05T01:22:25.127403Z",
     "start_time": "2020-11-05T01:22:23.363009Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/labbot/opt/anaconda3/lib/python3.8/site-packages/sklearn/feature_extraction/text.py:383: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ll', 've'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Topic  0\n",
      "baby, home, life, pup, animal, vet, shelter, toy, friend, pibble, face, pittie, mix, food, training\n",
      "\n",
      "Topic  1\n",
      "toy, ball, bone, antler, chewer, tooth, rope, toy toy, long, rubber, treat, chew, kong, hour, goughnut\n",
      "\n",
      "Topic  2\n",
      "food, vet, ear, allergy, grain, skin, diet, chicken, tail, infection, oil, heart, issue, meat, baby\n",
      "\n",
      "Topic  3\n",
      "collar, food, harness, training, vet, allergy, skin, reinforcement, grain, neck, issue, diet, properly, behavior, tool\n",
      "\n",
      "Topic  4\n",
      "ear, tail, animal, reason, breeder, child, mix, argument, toy, point, infection, actually, ear ear, fact, shelter\n",
      "\n",
      "Topic  5\n",
      "cat, animal, food, shelter, mix, issue, aggression, park, kid, attack, allergy, grain, home, fact, breeder\n",
      "\n",
      "Topic  6\n",
      "cat, home, ear, shelter, cat cat, rescue, crate, life, training, treat, collar, tail, bed, toy, forever\n",
      "\n",
      "Topic  7\n",
      "cat, face, pibble, bed, park, pittie, blanket, foot, hiking, hike, couch, pup, baby, mix, pretty\n",
      "\n",
      "Topic  8\n",
      "shelter, home, bed, rescue, blanket, breeder, crate, mix, couch, pup, house, definitely, work, pillow, vet\n",
      "\n",
      "Topic  9\n",
      "park, life, food, bed, friend, ear, training, home, advice, heart, family, dad, mom, socialization, pibble\n",
      "\n",
      "Topic  10\n",
      "vet, park, pup, nail, treat, home, place, advice, training, daycare, leash, friend, walk, surgery, crate\n",
      "\n",
      "Topic  11\n",
      "baby, child, food, training, shelter, park, animal, kid, treat, soon, recovery, behavior, pup, face, trainer\n",
      "\n",
      "Topic  12\n",
      "shelter, park, nail, face, breeder, pup, treat, harness, hiking, hike, food, mile, leash, friend, rescue\n",
      "\n",
      "Topic  13\n",
      "nail, animal, treat, life, harness, bed, breeder, foot, child, point, blanket, clipper, paw, kid, reason\n",
      "\n",
      "Topic  14\n",
      "hiking, hike, mile, walk, boot, trail, leash, water, pretty, outside, heat, foot, animal, baby, far\n"
     ]
    }
   ],
   "source": [
    "tighter_pitbull_topics = topic_model(data=tighter_data,\n",
    "                             vectorizer=CountVectorizer(stop_words=stop_words,min_df=10,ngram_range=(1, 2)),\n",
    "                            model='lsa',num_topics=15)\n",
    "pitbull_topics_cv_lsa = tighter_pitbull_topics.get_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-05T01:25:44.779734Z",
     "start_time": "2020-11-05T01:24:54.341471Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/labbot/opt/anaconda3/lib/python3.8/site-packages/sklearn/feature_extraction/text.py:383: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ll', 've'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Topic  0\n",
      "vet, home, shelter, surgery, rescue, pup, baby, soon, leg, recovery, care, work, pain, hour, definitely\n",
      "\n",
      "Topic  1\n",
      "picture, pic, photo, nail, pup, mom, baby, job, paw, video, shot, friend, phone, pretty, pittie\n",
      "\n",
      "Topic  2\n",
      "baby, pup, face, cutie, lab, mix, definitely, treat, god, goodness, sweetie, guy, pibble, absolutely, sweetheart\n",
      "\n",
      "Topic  3\n",
      "face, smile, seal, kiss, pupper, pittie, pibble, land, head, snoot, pup, tongue, land seal, cutie, baby\n",
      "\n",
      "Topic  4\n",
      "training, collar, park, leash, harness, behavior, walk, home, treat, trainer, away, pup, shelter, issue, advice\n",
      "\n",
      "Topic  5\n",
      "kid, animal, child, place, fact, person, mix, actually, attack, point, law, bite, type, issue, insurance\n",
      "\n",
      "Topic  6\n",
      "cat, animal, pibble, space, bug, service, squirrel, door, pittie, cat cat, house, human, bathroom, friend, actually\n",
      "\n",
      "Topic  7\n",
      "mix, pound, terrier, bully, lbs, lb, head, pretty, car, lap, test, pittie, definitely, dna, pibble\n",
      "\n",
      "Topic  8\n",
      "door, head, sweater, snow, outside, coat, boi, house, foot, rain, anxiety, guy, inside, winter, yeah\n",
      "\n",
      "Topic  9\n",
      "life, heart, home, friend, baby, family, hug, guy, pup, ago, forever, pibble, congrat, cancer, world\n",
      "\n",
      "Topic  10\n",
      "ear, tail, breeder, reason, ear ear, pittie, bat, actually, shit, rescue, infection, friend, fighting, shirt, shelter\n",
      "\n",
      "Topic  11\n",
      "treat, butter, peanut, pibble, cheese, peanut butter, walk, pup, ice, cream, mile, neck, hard, bag, water\n",
      "\n",
      "Topic  12\n",
      "toy, bed, blanket, couch, ball, pillow, bone, night, room, house, pibble, pittie, floor, long, rope\n",
      "\n",
      "Topic  13\n",
      "eye, nose, face, spot, pibble, head, absolutely, baby, color, exactly, pup, butt, pretty, actually, ya\n",
      "\n",
      "Topic  14\n",
      "food, allergy, vet, skin, water, bath, belly, chicken, diet, oil, grain, grass, issue, problem, bowl\n"
     ]
    }
   ],
   "source": [
    "tighter_pitbull_topics = topic_model(data=tighter_data,\n",
    "                             vectorizer=CountVectorizer(stop_words=stop_words,min_df=10,ngram_range=(1, 2)),\n",
    "                            model='lda',num_topics=15)\n",
    "pitbull_topics_cv_lda = tighter_pitbull_topics.get_topics()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LDA Viz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-05T02:03:32.250078Z",
     "start_time": "2020-11-05T02:03:32.102838Z"
    }
   },
   "outputs": [],
   "source": [
    "import pyLDAvis\n",
    "import pyLDAvis.sklearn\n",
    "pyLDAvis.disable_notebook()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-05T02:12:03.476981Z",
     "start_time": "2020-11-05T02:10:44.257219Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/labbot/opt/anaconda3/lib/python3.8/site-packages/sklearn/feature_extraction/text.py:383: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ll', 've'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n"
     ]
    }
   ],
   "source": [
    "# create pyLDAvis with countvectorizer, looser data, and save to html file\n",
    "cv = CountVectorizer(stop_words=stop_words,min_df=10,ngram_range=(1, 2))\n",
    "dtm_cv = cv.fit_transform(df['spacy_pipe'])\n",
    "\n",
    "lda_cv = LatentDirichletAllocation(n_components=15, random_state=0)\n",
    "lda_cv.fit(dtm_cv)\n",
    "\n",
    "lda_cv_viz = pyLDAvis.sklearn.prepare(lda_cv, dtm_cv, cv) \n",
    "pyLDAvis.save_html(lda_cv_viz,'lda_cv_viz.html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-05T02:13:01.275402Z",
     "start_time": "2020-11-05T02:12:03.479332Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/labbot/opt/anaconda3/lib/python3.8/site-packages/sklearn/feature_extraction/text.py:383: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ll', 've'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n"
     ]
    }
   ],
   "source": [
    "# create pyLDAvis with countvectorizer, tighter data, and save to html file\n",
    "cv = CountVectorizer(stop_words=stop_words,min_df=10,ngram_range=(1, 2))\n",
    "dtm_cv = cv.fit_transform(tighter_data)\n",
    "\n",
    "lda_cv = LatentDirichletAllocation(n_components=15, random_state=0)\n",
    "lda_cv.fit(dtm_cv)\n",
    "\n",
    "lda_cv_viz = pyLDAvis.sklearn.prepare(lda_cv, dtm_cv, cv) \n",
    "pyLDAvis.save_html(lda_cv_viz,'tight_lda_cv_viz.html')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "277px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
